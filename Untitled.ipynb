{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "\n",
    "from loss_function import gradient_norm_loss\n",
    "from Constant import C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_node):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,num_node)\n",
    "        self.fc2 = nn.Linear(num_node, num_node)\n",
    "        self.fc3 = nn.Linear(num_node, num_node)\n",
    "        self.fc3 = nn.Linear(num_node, num_node)\n",
    "        self.fc4 = nn.Linear(num_node, num_node)\n",
    "        self.fc5 = nn.Linear(num_node, num_node)\n",
    "        self.fc6 = nn.Linear(num_node, num_node)\n",
    "        self.fc7 = nn.Linear(num_node, num_node)\n",
    "        self.fc8 = nn.Linear(num_node, num_node)\n",
    "        self.fc9 = nn.Linear(num_node, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x = torch([batch, t, x])\n",
    "        '''\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        x = torch.tanh(self.fc5(x))\n",
    "        x = torch.tanh(self.fc6(x))\n",
    "        x = torch.tanh(self.fc7(x))\n",
    "        x = torch.tanh(self.fc8(x))\n",
    "        x = self.fc9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset, sample_batch_size, boundary_batch_size, \\\n",
    "         model, learning_rate, sample_loss_weight, gradient_loss_weight, max_step):\n",
    "    \n",
    "    \n",
    "\n",
    "    #model = Model()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    grad_loss = gradient_norm_loss()\n",
    "\n",
    "    params = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            params += p.numel()\n",
    "    print('params: ', params)\n",
    "\n",
    "    #writer = SummaryWriter(log_dir='./logs')\n",
    "\n",
    "    bc_and_init = define_bc_and_init(dataset)\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        #boundary_and_initial calucrate\n",
    "        boundary_input, boundary_label = get_boudary_point(bc_and_init, dataset, boundary_batch_size)\n",
    "        u = model(boundary_input)\n",
    "        loss_1 = criterion(u, boundary_label)\n",
    "\n",
    "        #sample calucurate\n",
    "        sample_input, _ = get_sample_point(dataset, sample_batch_size)\n",
    "        f_list, grad = f(sample_input, model)\n",
    "        fnc = torch.cat(f_list, dim=0).unsqueeze(1)\n",
    "        loss_2 = criterion(fnc, torch.zeros_like(fnc))\n",
    "        loss_3 = grad_loss(grad)\n",
    "\n",
    "        loss = loss_1 + sample_loss_weight * loss_2 + gradient_loss_weight * loss_3\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('step {} : loss {}'.format(step, loss))\n",
    "        #writer.add_scalar('Loss/step',loss, step)\n",
    "        #if step % 50 == 0:\n",
    "            #evaluate(model, dataset, step, writer)\n",
    "            #torch.save(model.state_dict(), 'checkpoint/step_{}.pth'.format(step))\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    L2_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for t_index, t in enumerate(dataset['t']):\n",
    "            for x_index, x in enumerate(dataset['x']):\n",
    "                #print('t', t.shape)\n",
    "                #print('x', x)\n",
    "                input_array = torch.tensor(np.concatenate([t, x]), dtype=torch.float32)\n",
    "                #print(input_array)\n",
    "                pred_u = model(input_array)\n",
    "                L2_loss += (pred_u.item() - dataset['u'][x_index][0][t_index]) ** 2\n",
    "    #writer.add_scalar('pre_L2loss/step', L2_loss.item(), step)\n",
    "    #print('pre_L2loss: {}'.format(L2_loss.item()))\n",
    "    model.train()\n",
    "    return L2_loss\n",
    "\n",
    "def define_bc_and_init(dataset):\n",
    "    max_time_step = dataset['t'].shape[0]\n",
    "    left_pos_index = 0\n",
    "    right_pos_index = dataset['x'].shape[0]\n",
    "    left_bc = np.concatenate([np.arange(1, max_time_step, dtype=np.int32).reshape(max_time_step - 1, 1), \\\n",
    "                              np.full((max_time_step - 1, 1), left_pos_index)], axis=1)\n",
    "    #print(left_bc)\n",
    "    right_bc = np.concatenate([np.arange(1, max_time_step, dtype=np.int32).reshape(max_time_step - 1, 1), \\\n",
    "                              np.full((max_time_step - 1, 1), right_pos_index - 1)], axis=1)\n",
    "    #print(right_bc)\n",
    "    initial_con = np.concatenate([np.full((right_pos_index, 1), 0), \\\n",
    "                                  np.arange(right_pos_index, dtype=np.int32).reshape(right_pos_index, 1)], axis=1)\n",
    "    #print(initial_con)\n",
    "    #print('*'*80)\n",
    "    return np.concatenate([left_bc, right_bc, initial_con], axis=0)\n",
    "\n",
    "def f(sample_input, model):\n",
    "    f_list = []\n",
    "    grad_list = []\n",
    "    for one_sample in sample_input:\n",
    "        t = one_sample[0].unsqueeze(0)\n",
    "        x = one_sample[1].unsqueeze(0)\n",
    "        u = model(torch.cat([t, x], dim=0))\n",
    "        gradient = torch.autograd.grad(u, (t, x), retain_graph=True, create_graph=True)\n",
    "        f = gradient[0] + C * gradient[1]\n",
    "        f_list.append(f)\n",
    "        grad_list.extend([gradient[0], gradient[1]])\n",
    "    return f_list, torch.tensor(grad_list, dtype=torch.float32)\n",
    "    \n",
    "def data_load(data_path):\n",
    "    with open(data_path,mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def get_label_list(label, batch_point):\n",
    "    '''\n",
    "    label [time * stencil]\n",
    "    batch_point [batch_size * 2(time, point)]\n",
    "    '''\n",
    "    label_list = []\n",
    "    for point in batch_point:\n",
    "        value = torch.unsqueeze(label[point[0], point[1]], 0)\n",
    "        label_list.append(value)\n",
    "    return label_list\n",
    "\n",
    "def get_sample_point(dataset, sample_batch_size):\n",
    "    time_step = np.random.randint(1, dataset['t'].shape[0], (sample_batch_size, 1))\n",
    "    #time_step = np.array([0])\n",
    "    #time_step = time_step.astype(np.float32)\n",
    "    sample_stencil = np.random.randint(1, dataset['x'].shape[0] - 2, (sample_batch_size, 1))\n",
    "    #sample_stencil = sample_stencil.astype(np.float32)\n",
    "    pair_batch = np.concatenate([time_step, sample_stencil], axis=1)\n",
    "    #print(\"pair_batch\", pair_batch)\n",
    "    batch_list = []\n",
    "    for pair in pair_batch:\n",
    "        one_dataset = np.concatenate([dataset['t'][pair[0]], \\\n",
    "                                      dataset['x'][pair[1]], \\\n",
    "                                      np.array([dataset['u'][pair[1]][0][pair[0]]])])                             \n",
    "        batch_list.append(one_dataset)\n",
    "    numerical_data = np.stack(batch_list, axis=0)\n",
    "    #print('numerical data', numerical_data)\n",
    "    return torch.tensor(numerical_data[:, 0:2], dtype=torch.float32, requires_grad=True), torch.tensor(numerical_data[:, 2], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "def get_boudary_point(bc_and_init, dataset, boundary_batch_size):\n",
    "\n",
    "    index_list = np.random.choice(range(bc_and_init.shape[0]), boundary_batch_size)\n",
    "    pair_batch = np.stack([bc_and_init[index] for index in index_list], axis=0)\n",
    "\n",
    "    batch_list = []\n",
    "    for pair in pair_batch:\n",
    "        one_dataset = np.concatenate([dataset['t'][pair[0]], \\\n",
    "                                      dataset['x'][pair[1]], \\\n",
    "                                      np.array([dataset['u'][pair[1]][0][pair[0]]])])                             \n",
    "        batch_list.append(one_dataset)\n",
    "\n",
    "    numerical_data = np.stack(batch_list, axis=0)\n",
    "    return torch.tensor(numerical_data[:, 0:2], dtype=torch.float32, requires_grad=True), torch.tensor(numerical_data[:, 2], dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    num_node = trial.suggest_int('num_nodes', 20, 100)\n",
    "    learning_rate = trial.suggest_uniform('lr', 1e-4, 1e-1)\n",
    "    sample_batch_size = trial.suggest_int('sample_batch_size', 500, 8000)\n",
    "    boundary_batch_size = trial.suggest_int('boundary_batch_size', 20, 300)\n",
    "    gradient_loss_weight = trial.suggest_uniform('gradient_loss_weight', 1e-2, 10)\n",
    "    sample_loss_weight = trial.suggest_uniform('sample_loss_weight', 1e-2, 10)\n",
    "    \n",
    "    model = Model(num_node)\n",
    "    dataset = data_load('dataset.pkl')\n",
    "    \n",
    "    main(dataset, sample_batch_size, boundary_batch_size, \\\n",
    "          model, learning_rate, sample_loss_weight, gradient_loss_weight, max_step=3000)\n",
    "    L2_loss = evaluate(model, dataset)\n",
    "    return L2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-08 14:41:14,183] A new study created in memory with name: no-name-43ec2609-0507-4494-8614-fdcef34a5f5b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:  53941\n",
      "step 0 : loss 3.196103096008301\n",
      "step 1 : loss 3.1523444652557373\n",
      "step 2 : loss 3.2860267162323\n",
      "step 3 : loss 3.3421390056610107\n",
      "step 4 : loss 3.385719060897827\n",
      "step 5 : loss 3.4026870727539062\n",
      "step 6 : loss 3.4101169109344482\n",
      "step 7 : loss 3.5189192295074463\n",
      "step 8 : loss 3.6040737628936768\n",
      "step 9 : loss 3.4695887565612793\n",
      "step 10 : loss 3.568143367767334\n",
      "step 11 : loss 3.660487413406372\n",
      "step 12 : loss 3.7853286266326904\n",
      "step 13 : loss 3.7643373012542725\n",
      "step 14 : loss 3.8444013595581055\n",
      "step 15 : loss 3.9822275638580322\n",
      "step 16 : loss 3.858593463897705\n",
      "step 17 : loss 4.00861120223999\n",
      "step 18 : loss 4.114837169647217\n",
      "step 19 : loss 4.1826043128967285\n",
      "step 20 : loss 4.115908145904541\n",
      "step 21 : loss 4.2595534324646\n",
      "step 22 : loss 4.225491046905518\n",
      "step 23 : loss 4.322822570800781\n",
      "step 24 : loss 4.376670837402344\n",
      "step 25 : loss 4.493327617645264\n",
      "step 26 : loss 4.465932846069336\n",
      "step 27 : loss 4.804269313812256\n",
      "step 28 : loss 4.899441242218018\n",
      "step 29 : loss 4.687672138214111\n",
      "step 30 : loss 4.705150604248047\n",
      "step 31 : loss 5.1212029457092285\n",
      "step 32 : loss 4.8859357833862305\n",
      "step 33 : loss 4.967839241027832\n",
      "step 34 : loss 4.962865829467773\n",
      "step 35 : loss 5.244739532470703\n",
      "step 36 : loss 5.210681915283203\n",
      "step 37 : loss 5.276119709014893\n",
      "step 38 : loss 5.242499828338623\n",
      "step 39 : loss 5.582785606384277\n",
      "step 40 : loss 5.363580226898193\n",
      "step 41 : loss 5.422157287597656\n",
      "step 42 : loss 5.5848307609558105\n",
      "step 43 : loss 5.763399600982666\n",
      "step 44 : loss 5.865156650543213\n",
      "step 45 : loss 5.641852378845215\n",
      "step 46 : loss 5.821645736694336\n",
      "step 47 : loss 5.769381523132324\n",
      "step 48 : loss 5.741170883178711\n",
      "step 49 : loss 6.106293678283691\n",
      "step 50 : loss 5.891847133636475\n",
      "step 51 : loss 6.0520853996276855\n",
      "step 52 : loss 6.156085014343262\n",
      "step 53 : loss 6.122433185577393\n",
      "step 54 : loss 6.1941914558410645\n",
      "step 55 : loss 6.110351085662842\n",
      "step 56 : loss 6.205341339111328\n",
      "step 57 : loss 6.148994445800781\n",
      "step 58 : loss 6.24884557723999\n",
      "step 59 : loss 6.48854398727417\n",
      "step 60 : loss 6.330239295959473\n",
      "step 61 : loss 6.3652496337890625\n",
      "step 62 : loss 6.367241382598877\n",
      "step 63 : loss 6.557684898376465\n",
      "step 64 : loss 6.57284688949585\n",
      "step 65 : loss 6.63905143737793\n",
      "step 66 : loss 6.582668781280518\n",
      "step 67 : loss 6.463943958282471\n",
      "step 68 : loss 6.8143157958984375\n",
      "step 69 : loss 6.651561737060547\n",
      "step 70 : loss 6.45190954208374\n",
      "step 71 : loss 6.482935905456543\n",
      "step 72 : loss 6.717483997344971\n",
      "step 73 : loss 6.5698628425598145\n",
      "step 74 : loss 6.687771797180176\n",
      "step 75 : loss 6.522043228149414\n",
      "step 76 : loss 6.519400119781494\n",
      "step 77 : loss 6.532005310058594\n",
      "step 78 : loss 6.547104835510254\n",
      "step 79 : loss 6.631536960601807\n",
      "step 80 : loss 6.428947925567627\n",
      "step 81 : loss 6.518627166748047\n",
      "step 82 : loss 6.219977378845215\n",
      "step 83 : loss 6.432674407958984\n",
      "step 84 : loss 6.255564212799072\n",
      "step 85 : loss 6.3901801109313965\n",
      "step 86 : loss 6.313771724700928\n",
      "step 87 : loss 6.293483257293701\n",
      "step 88 : loss 6.154352188110352\n",
      "step 89 : loss 6.08103084564209\n",
      "step 90 : loss 6.0874342918396\n",
      "step 91 : loss 6.045867443084717\n",
      "step 92 : loss 6.023703575134277\n",
      "step 93 : loss 5.886472225189209\n",
      "step 94 : loss 5.976247787475586\n",
      "step 95 : loss 5.959016799926758\n",
      "step 96 : loss 5.84910249710083\n",
      "step 97 : loss 5.79464864730835\n",
      "step 98 : loss 5.752705097198486\n",
      "step 99 : loss 5.681584358215332\n",
      "step 100 : loss 5.502142429351807\n",
      "step 101 : loss 5.647279262542725\n",
      "step 102 : loss 5.520264148712158\n",
      "step 103 : loss 5.50258731842041\n",
      "step 104 : loss 5.379683017730713\n",
      "step 105 : loss 5.406081199645996\n",
      "step 106 : loss 5.267394542694092\n",
      "step 107 : loss 5.278746128082275\n",
      "step 108 : loss 5.3213114738464355\n",
      "step 109 : loss 5.23895788192749\n",
      "step 110 : loss 5.13538122177124\n",
      "step 111 : loss 5.143949508666992\n",
      "step 112 : loss 5.126792907714844\n",
      "step 113 : loss 5.086360454559326\n",
      "step 114 : loss 4.907759666442871\n",
      "step 115 : loss 4.752073287963867\n",
      "step 116 : loss 4.870856285095215\n",
      "step 117 : loss 4.798993110656738\n",
      "step 118 : loss 4.786185264587402\n",
      "step 119 : loss 4.619542121887207\n",
      "step 120 : loss 4.602473258972168\n",
      "step 121 : loss 4.576590538024902\n",
      "step 122 : loss 4.499614238739014\n",
      "step 123 : loss 4.437211036682129\n",
      "step 124 : loss 4.441826343536377\n",
      "step 125 : loss 4.325348854064941\n",
      "step 126 : loss 4.3038811683654785\n",
      "step 127 : loss 4.313435077667236\n",
      "step 128 : loss 4.258223056793213\n",
      "step 129 : loss 4.220436096191406\n",
      "step 130 : loss 4.126805305480957\n",
      "step 131 : loss 4.332713603973389\n",
      "step 132 : loss 3.9732279777526855\n",
      "step 133 : loss 4.176893711090088\n",
      "step 134 : loss 3.9267807006835938\n",
      "step 135 : loss 3.877624988555908\n",
      "step 136 : loss 3.9250214099884033\n",
      "step 137 : loss 4.007082462310791\n",
      "step 138 : loss 3.9291915893554688\n",
      "step 139 : loss 3.8563942909240723\n",
      "step 140 : loss 3.811028003692627\n",
      "step 141 : loss 3.8116588592529297\n",
      "step 142 : loss 3.7608213424682617\n",
      "step 143 : loss 3.6997463703155518\n",
      "step 144 : loss 3.6297054290771484\n",
      "step 145 : loss 3.657773494720459\n",
      "step 146 : loss 3.5752933025360107\n",
      "step 147 : loss 3.497020721435547\n",
      "step 148 : loss 3.5247466564178467\n",
      "step 149 : loss 3.6026062965393066\n",
      "step 150 : loss 3.6011641025543213\n",
      "step 151 : loss 3.5164315700531006\n",
      "step 152 : loss 3.446373462677002\n",
      "step 153 : loss 3.2142837047576904\n",
      "step 154 : loss 3.3412156105041504\n",
      "step 155 : loss 3.3752312660217285\n",
      "step 156 : loss 3.2436225414276123\n",
      "step 157 : loss 3.2871453762054443\n",
      "step 158 : loss 3.3405067920684814\n",
      "step 159 : loss 3.2209644317626953\n",
      "step 160 : loss 3.136214256286621\n",
      "step 161 : loss 3.148298740386963\n",
      "step 162 : loss 3.2024426460266113\n",
      "step 163 : loss 3.127197742462158\n",
      "step 164 : loss 3.077224016189575\n",
      "step 165 : loss 2.949333906173706\n",
      "step 166 : loss 2.955129861831665\n",
      "step 167 : loss 3.001568078994751\n",
      "step 168 : loss 3.021005868911743\n",
      "step 169 : loss 2.9911916255950928\n",
      "step 170 : loss 2.8850066661834717\n",
      "step 171 : loss 2.9711220264434814\n",
      "step 172 : loss 2.864048957824707\n",
      "step 173 : loss 2.87528657913208\n",
      "step 174 : loss 2.84552264213562\n",
      "step 175 : loss 2.764352321624756\n",
      "step 176 : loss 2.7612690925598145\n",
      "step 177 : loss 2.6902549266815186\n",
      "step 178 : loss 2.7417283058166504\n",
      "step 179 : loss 2.8310739994049072\n",
      "step 180 : loss 2.683424472808838\n",
      "step 181 : loss 2.690570592880249\n",
      "step 182 : loss 2.6333494186401367\n",
      "step 183 : loss 2.7595086097717285\n",
      "step 184 : loss 2.6252048015594482\n",
      "step 185 : loss 2.7179441452026367\n",
      "step 186 : loss 2.6269314289093018\n",
      "step 187 : loss 2.6714282035827637\n",
      "step 188 : loss 2.5827765464782715\n",
      "step 189 : loss 2.562101125717163\n",
      "step 190 : loss 2.4562180042266846\n",
      "step 191 : loss 2.4758188724517822\n",
      "step 192 : loss 2.5476787090301514\n",
      "step 193 : loss 2.447868824005127\n",
      "step 194 : loss 2.4669132232666016\n",
      "step 195 : loss 2.342243194580078\n",
      "step 196 : loss 2.4317755699157715\n",
      "step 197 : loss 2.3925108909606934\n",
      "step 198 : loss 2.462419271469116\n",
      "step 199 : loss 2.3453586101531982\n",
      "step 200 : loss 2.391237258911133\n",
      "step 201 : loss 2.325343608856201\n",
      "step 202 : loss 2.3281023502349854\n",
      "step 203 : loss 2.4188590049743652\n",
      "step 204 : loss 2.245448350906372\n",
      "step 205 : loss 2.275684356689453\n",
      "step 206 : loss 2.1808578968048096\n",
      "step 207 : loss 2.3117167949676514\n",
      "step 208 : loss 2.1948769092559814\n",
      "step 209 : loss 2.154538869857788\n",
      "step 210 : loss 2.2185943126678467\n",
      "step 211 : loss 2.172245502471924\n",
      "step 212 : loss 2.109506845474243\n",
      "step 213 : loss 2.1566712856292725\n",
      "step 214 : loss 2.0882604122161865\n",
      "step 215 : loss 2.1186203956604004\n",
      "step 216 : loss 2.0734283924102783\n",
      "step 217 : loss 1.9870531558990479\n",
      "step 218 : loss 2.058729887008667\n",
      "step 219 : loss 2.0703752040863037\n",
      "step 220 : loss 2.0924253463745117\n",
      "step 221 : loss 2.03617525100708\n",
      "step 222 : loss 2.0582945346832275\n",
      "step 223 : loss 1.9720052480697632\n",
      "step 224 : loss 1.9972844123840332\n",
      "step 225 : loss 1.9323023557662964\n",
      "step 226 : loss 1.9698232412338257\n",
      "step 227 : loss 1.9863680601119995\n",
      "step 228 : loss 2.03761625289917\n",
      "step 229 : loss 1.9090328216552734\n",
      "step 230 : loss 1.942599892616272\n",
      "step 231 : loss 1.9469717741012573\n",
      "step 232 : loss 1.9399391412734985\n",
      "step 233 : loss 1.8729313611984253\n",
      "step 234 : loss 1.798362135887146\n",
      "step 235 : loss 1.794650912284851\n",
      "step 236 : loss 1.8541278839111328\n",
      "step 237 : loss 1.845622181892395\n",
      "step 238 : loss 1.8486661911010742\n",
      "step 239 : loss 1.8639894723892212\n",
      "step 240 : loss 1.7754886150360107\n",
      "step 241 : loss 1.7622240781784058\n",
      "step 242 : loss 1.795562982559204\n",
      "step 243 : loss 1.809342622756958\n",
      "step 244 : loss 1.7273393869400024\n",
      "step 245 : loss 1.7296522855758667\n",
      "step 246 : loss 1.7304435968399048\n",
      "step 247 : loss 1.8664369583129883\n",
      "step 248 : loss 1.7392815351486206\n",
      "step 249 : loss 1.710992455482483\n",
      "step 250 : loss 1.6650934219360352\n",
      "step 251 : loss 1.700440526008606\n",
      "step 252 : loss 1.6732642650604248\n",
      "step 253 : loss 1.692875862121582\n",
      "step 254 : loss 1.6402480602264404\n",
      "step 255 : loss 1.6735996007919312\n",
      "step 256 : loss 1.6829313039779663\n",
      "step 257 : loss 1.6633459329605103\n",
      "step 258 : loss 1.7606549263000488\n",
      "step 259 : loss 1.6361411809921265\n",
      "step 260 : loss 1.5587761402130127\n",
      "step 261 : loss 1.5950994491577148\n",
      "step 262 : loss 1.573306918144226\n",
      "step 263 : loss 1.6228504180908203\n",
      "step 264 : loss 1.5566976070404053\n",
      "step 265 : loss 1.6011183261871338\n",
      "step 266 : loss 1.5257068872451782\n",
      "step 267 : loss 1.5342514514923096\n",
      "step 268 : loss 1.493524193763733\n",
      "step 269 : loss 1.5383200645446777\n",
      "step 270 : loss 1.5854825973510742\n",
      "step 271 : loss 1.548828363418579\n",
      "step 272 : loss 1.428200602531433\n",
      "step 273 : loss 1.53251314163208\n",
      "step 274 : loss 1.4798429012298584\n",
      "step 275 : loss 1.5417377948760986\n",
      "step 276 : loss 1.4847545623779297\n",
      "step 277 : loss 1.5208202600479126\n",
      "step 278 : loss 1.5098199844360352\n",
      "step 279 : loss 1.4752326011657715\n",
      "step 280 : loss 1.4575471878051758\n",
      "step 281 : loss 1.438739538192749\n",
      "step 282 : loss 1.4539997577667236\n",
      "step 283 : loss 1.4466906785964966\n",
      "step 284 : loss 1.420192837715149\n",
      "step 285 : loss 1.4277862310409546\n",
      "step 286 : loss 1.465819239616394\n",
      "step 287 : loss 1.4132755994796753\n",
      "step 288 : loss 1.3852282762527466\n",
      "step 289 : loss 1.3644198179244995\n",
      "step 290 : loss 1.4162033796310425\n",
      "step 291 : loss 1.3802911043167114\n",
      "step 292 : loss 1.4062294960021973\n",
      "step 293 : loss 1.416396975517273\n",
      "step 294 : loss 1.326296091079712\n",
      "step 295 : loss 1.3839960098266602\n",
      "step 296 : loss 1.3429814577102661\n",
      "step 297 : loss 1.3515806198120117\n",
      "step 298 : loss 1.327636480331421\n",
      "step 299 : loss 1.4037001132965088\n",
      "step 300 : loss 1.324000358581543\n",
      "step 301 : loss 1.2945590019226074\n",
      "step 302 : loss 1.2774651050567627\n",
      "step 303 : loss 1.3167973756790161\n",
      "step 304 : loss 1.3194693326950073\n",
      "step 305 : loss 1.3183928728103638\n",
      "step 306 : loss 1.301775574684143\n",
      "step 307 : loss 1.288076400756836\n",
      "step 308 : loss 1.213392972946167\n",
      "step 309 : loss 1.2912862300872803\n",
      "step 310 : loss 1.2905967235565186\n",
      "step 311 : loss 1.2983415126800537\n",
      "step 312 : loss 1.2144038677215576\n",
      "step 313 : loss 1.2286250591278076\n",
      "step 314 : loss 1.2400729656219482\n",
      "step 315 : loss 1.2942500114440918\n",
      "step 316 : loss 1.2348068952560425\n",
      "step 317 : loss 1.2216804027557373\n",
      "step 318 : loss 1.2226091623306274\n",
      "step 319 : loss 1.1873502731323242\n",
      "step 320 : loss 1.2402044534683228\n",
      "step 321 : loss 1.2318429946899414\n",
      "step 322 : loss 1.26997709274292\n",
      "step 323 : loss 1.2264567613601685\n",
      "step 324 : loss 1.2008689641952515\n",
      "step 325 : loss 1.1939207315444946\n",
      "step 326 : loss 1.2236824035644531\n",
      "step 327 : loss 1.1997463703155518\n",
      "step 328 : loss 1.1633297204971313\n",
      "step 329 : loss 1.1676421165466309\n",
      "step 330 : loss 1.1620962619781494\n",
      "step 331 : loss 1.146560788154602\n",
      "step 332 : loss 1.1389756202697754\n",
      "step 333 : loss 1.1856184005737305\n",
      "step 334 : loss 1.1293327808380127\n",
      "step 335 : loss 1.1680567264556885\n",
      "step 336 : loss 1.1289336681365967\n",
      "step 337 : loss 1.1568026542663574\n",
      "step 338 : loss 1.1237701177597046\n",
      "step 339 : loss 1.1315762996673584\n",
      "step 340 : loss 1.131485104560852\n",
      "step 341 : loss 1.1546390056610107\n",
      "step 342 : loss 1.1786079406738281\n",
      "step 343 : loss 1.0933510065078735\n",
      "step 344 : loss 1.067822813987732\n",
      "step 345 : loss 1.0876858234405518\n",
      "step 346 : loss 1.099113941192627\n",
      "step 347 : loss 1.1142032146453857\n",
      "step 348 : loss 1.1031461954116821\n",
      "step 349 : loss 1.0856294631958008\n",
      "step 350 : loss 1.10258948802948\n",
      "step 351 : loss 1.087599277496338\n",
      "step 352 : loss 1.1110625267028809\n",
      "step 353 : loss 1.0150940418243408\n",
      "step 354 : loss 1.0637638568878174\n",
      "step 355 : loss 1.089490532875061\n",
      "step 356 : loss 1.0639091730117798\n",
      "step 357 : loss 1.0794677734375\n",
      "step 358 : loss 1.0052167177200317\n",
      "step 359 : loss 1.0789738893508911\n",
      "step 360 : loss 1.0525016784667969\n",
      "step 361 : loss 1.1078674793243408\n",
      "step 362 : loss 1.0105512142181396\n",
      "step 363 : loss 1.0077903270721436\n",
      "step 364 : loss 1.0507242679595947\n",
      "step 365 : loss 1.0946588516235352\n",
      "step 366 : loss 1.0388638973236084\n",
      "step 367 : loss 1.0268782377243042\n",
      "step 368 : loss 0.9885498881340027\n",
      "step 369 : loss 1.0688024759292603\n",
      "step 370 : loss 1.0029412508010864\n",
      "step 371 : loss 1.0376930236816406\n",
      "step 372 : loss 0.9867815971374512\n",
      "step 373 : loss 1.0404399633407593\n",
      "step 374 : loss 0.9763391613960266\n",
      "step 375 : loss 0.9527382254600525\n",
      "step 376 : loss 0.9845147132873535\n",
      "step 377 : loss 1.010798692703247\n",
      "step 378 : loss 1.0037564039230347\n",
      "step 379 : loss 1.0365493297576904\n",
      "step 380 : loss 0.9894893169403076\n",
      "step 381 : loss 1.0351231098175049\n",
      "step 382 : loss 0.9928330779075623\n",
      "step 383 : loss 0.9337920546531677\n",
      "step 384 : loss 0.9867488145828247\n",
      "step 385 : loss 0.9613246321678162\n",
      "step 386 : loss 0.9753182530403137\n",
      "step 387 : loss 0.9989410042762756\n",
      "step 388 : loss 0.9294210076332092\n",
      "step 389 : loss 0.9264814853668213\n",
      "step 390 : loss 0.9155100584030151\n",
      "step 391 : loss 0.9493694305419922\n",
      "step 392 : loss 0.9118644595146179\n",
      "step 393 : loss 0.9522610902786255\n",
      "step 394 : loss 0.9291613698005676\n",
      "step 395 : loss 0.9393075704574585\n",
      "step 396 : loss 0.9347741603851318\n",
      "step 397 : loss 0.9282546043395996\n",
      "step 398 : loss 0.9464776515960693\n",
      "step 399 : loss 0.9462547898292542\n",
      "step 400 : loss 0.9028480052947998\n",
      "step 401 : loss 0.8831594586372375\n",
      "step 402 : loss 0.9153974056243896\n",
      "step 403 : loss 0.8798827528953552\n",
      "step 404 : loss 0.8627683520317078\n",
      "step 405 : loss 0.9370157718658447\n",
      "step 406 : loss 0.8647298216819763\n",
      "step 407 : loss 0.9177730083465576\n",
      "step 408 : loss 0.85051029920578\n",
      "step 409 : loss 0.8956723213195801\n",
      "step 410 : loss 0.9074156284332275\n",
      "step 411 : loss 0.8807573318481445\n",
      "step 412 : loss 0.8601936101913452\n",
      "step 413 : loss 0.8434187173843384\n",
      "step 414 : loss 0.8743869066238403\n",
      "step 415 : loss 0.8729062080383301\n",
      "step 416 : loss 0.8955559134483337\n",
      "step 417 : loss 0.8802947998046875\n",
      "step 418 : loss 0.8503853678703308\n",
      "step 419 : loss 0.8726490139961243\n",
      "step 420 : loss 0.8857367038726807\n",
      "step 421 : loss 0.8731958270072937\n",
      "step 422 : loss 0.8629876971244812\n",
      "step 423 : loss 0.8348090648651123\n",
      "step 424 : loss 0.879535973072052\n",
      "step 425 : loss 0.8096678853034973\n",
      "step 426 : loss 0.8510794639587402\n",
      "step 427 : loss 0.8040761947631836\n",
      "step 428 : loss 0.8356001973152161\n",
      "step 429 : loss 0.8216526508331299\n",
      "step 430 : loss 0.8000379204750061\n",
      "step 431 : loss 0.8662793636322021\n",
      "step 432 : loss 0.8091205358505249\n",
      "step 433 : loss 0.8243021965026855\n",
      "step 434 : loss 0.7963789701461792\n",
      "step 435 : loss 0.8119509220123291\n",
      "step 436 : loss 0.8239924311637878\n",
      "step 437 : loss 0.8638961315155029\n",
      "step 438 : loss 0.7917261719703674\n",
      "step 439 : loss 0.8338712453842163\n",
      "step 440 : loss 0.810829758644104\n",
      "step 441 : loss 0.8771586418151855\n",
      "step 442 : loss 0.8117598295211792\n",
      "step 443 : loss 0.8562376499176025\n",
      "step 444 : loss 0.8489733338356018\n",
      "step 445 : loss 0.8053200244903564\n",
      "step 446 : loss 0.8016818165779114\n",
      "step 447 : loss 0.8407980799674988\n",
      "step 448 : loss 0.8117934465408325\n",
      "step 449 : loss 0.8215042352676392\n",
      "step 450 : loss 0.7915098667144775\n",
      "step 451 : loss 0.8127073645591736\n",
      "step 452 : loss 0.7786492109298706\n",
      "step 453 : loss 0.7854354381561279\n",
      "step 454 : loss 0.8392791152000427\n",
      "step 455 : loss 0.7875992655754089\n",
      "step 456 : loss 0.7789478898048401\n",
      "step 457 : loss 0.7919951677322388\n",
      "step 458 : loss 0.771696150302887\n",
      "step 459 : loss 0.7549868226051331\n",
      "step 460 : loss 0.7681244611740112\n",
      "step 461 : loss 0.7805449962615967\n",
      "step 462 : loss 0.7740312218666077\n",
      "step 463 : loss 0.7727614641189575\n",
      "step 464 : loss 0.7711760997772217\n",
      "step 465 : loss 0.7643991708755493\n",
      "step 466 : loss 0.7511865496635437\n",
      "step 467 : loss 0.7589343190193176\n",
      "step 468 : loss 0.7522504329681396\n",
      "step 469 : loss 0.7953981161117554\n",
      "step 470 : loss 0.801051139831543\n",
      "step 471 : loss 0.7805296778678894\n",
      "step 472 : loss 0.7895049452781677\n",
      "step 473 : loss 0.7964319586753845\n",
      "step 474 : loss 0.7068747878074646\n",
      "step 475 : loss 0.7546688318252563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 476 : loss 0.7315928936004639\n",
      "step 477 : loss 0.7865247130393982\n",
      "step 478 : loss 0.7225408554077148\n",
      "step 479 : loss 0.7452367544174194\n",
      "step 480 : loss 0.736639142036438\n",
      "step 481 : loss 0.7602383494377136\n",
      "step 482 : loss 0.7491892576217651\n",
      "step 483 : loss 0.7637051939964294\n",
      "step 484 : loss 0.733884334564209\n",
      "step 485 : loss 0.7325577735900879\n",
      "step 486 : loss 0.7562928199768066\n",
      "step 487 : loss 0.6814507842063904\n",
      "step 488 : loss 0.7324724197387695\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-910f311007d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/reinforce/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 )\n\u001b[1;32m    341\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/reinforce/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    745\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/reinforce/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    774\u001b[0m     ) -> None:\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/reinforce/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Trial {} pruned. {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-f8f42c3d59df>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     main(dataset, sample_batch_size, boundary_batch_size, \\\n\u001b[0;32m---> 13\u001b[0;31m           model, learning_rate, sample_loss_weight, gradient_loss_weight, max_step=10000)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mL2_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mL2_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-4d90e2623254>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataset, sample_batch_size, boundary_batch_size, model, learning_rate, sample_loss_weight, gradient_loss_weight, max_step)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_loss_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgradient_loss_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step {} : loss {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/reinforce/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/reinforce/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7dd2388d8dd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor([[1, 2], [4, 2], [6, 3]], dtype=torch.float32, requires_grad=True)\n",
    "#x_2 = torch.tensor([2, 4, 6], dtype=torch.float32, requires_grad=True)\n",
    "a = torch.tensor([7], dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor([11], dtype=torch.float32, requires_grad=True)\n",
    "c = torch.tensor([13], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = a * x_1[:, 0] + b * x_1[:, 1] + c +  x_1[:, 0] * x_1[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(y, dim=0).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16., 23.],\n",
       "        [16., 26.],\n",
       "        [17., 28.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11., 11., 11.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
